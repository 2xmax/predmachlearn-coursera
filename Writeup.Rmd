Human Activity Recognition
=========================

Motivation
------------------------------

The goal of this work is to predict human activities by collected data using devices such as Jawbone Up, Nike FuelBand, and Fitbit. The approach of research is based upon this paper <http://groupware.les.inf.puc-rio.br/har>.

Environment initialization
------------------------------
Let us load the following packages:
```{r}
library(dplyr)
library(caret)
library(randomForest)
```

And set some seed to make the experiment reproducible

```{r}
set.seed(42)
```

Obtaining and preprocessing data
------------------------------

```{r}
if (!file.exists("pml-training.csv")){
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv", method="curl")  
}

# remove N/A's and empty values from dataset
raw <- read.csv("pml-training.csv", na.strings = c("NA",""), stringsAsFactors = FALSE)
```

The original dataset have a lot of noise, let us clean it.
```{r}
cleanupData = function(raw){
  # remove unrelated fields from model
  cleaned <- subset(raw, select = -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window))
  # there are a lot of columns with missing data, throw them up
  naThreshold = as.integer(nrow(raw) * 0.5)
  cleaned <- subset(cleaned, select = -which(colSums(is.na(cleaned)) > naThreshold))
  cleaned
}

data = cleanupData(raw)
```

Prediction
------------------------------

Let us split train and test crossvalidation sets in typical proportion, let us say, in 70%/30%.
```{r}
data$classe <- as.factor(data$classe)
testIndex = createDataPartition(data$classe, p = 0.7, list=FALSE)
train = data[-testIndex,]
test = data[testIndex,]

```

For best accurancy without requirement of good interpretaion the best choice will be random forests algorithm (look for code.R for details).
```{r}
predictor <- randomForest(classe ~ ., data = train)
```

N.B.: for simplicity, single thread code has been provided. It can be run in parallel, using doParallel package (TBD: provide example).

Conclusion
------------------------------

```{r}
testPred = predict(predictor, newdata = test)
accurancy = confusionMatrix(test$classe, testPred)$overall[1]
```

It can be concluded that the algorithm's accurancy is equal to 98.37% and the out-of-sample error is 1.63%.
